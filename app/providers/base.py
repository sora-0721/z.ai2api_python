#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
åŸºç¡€æä¾›å•†æŠ½è±¡å±‚
å®šä¹‰ç»Ÿä¸€çš„æä¾›å•†æ¥å£è§„èŒƒ
"""

import json
import time
import uuid
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional, AsyncGenerator, Union
from dataclasses import dataclass

from app.models.schemas import OpenAIRequest, Message
from app.utils.logger import get_logger

logger = get_logger()


@dataclass
class ProviderConfig:
    """æä¾›å•†é…ç½®"""
    name: str
    api_endpoint: str
    timeout: int = 30
    headers: Optional[Dict[str, str]] = None
    extra_config: Optional[Dict[str, Any]] = None


@dataclass
class ProviderResponse:
    """æä¾›å•†å“åº”"""
    success: bool
    content: str = ""
    error: Optional[str] = None
    usage: Optional[Dict[str, int]] = None
    extra_data: Optional[Dict[str, Any]] = None


class BaseProvider(ABC):
    """åŸºç¡€æä¾›å•†æŠ½è±¡ç±»"""
    
    def __init__(self, config: ProviderConfig):
        """åˆå§‹åŒ–æä¾›å•†"""
        self.config = config
        self.name = config.name
        self.logger = get_logger()
        
    @abstractmethod
    async def chat_completion(
        self, 
        request: OpenAIRequest,
        **kwargs
    ) -> Union[Dict[str, Any], AsyncGenerator[str, None]]:
        """
        èŠå¤©å®Œæˆæ¥å£
        
        Args:
            request: OpenAIæ ¼å¼çš„è¯·æ±‚
            **kwargs: é¢å¤–å‚æ•°
            
        Returns:
            éæµå¼: Dict[str, Any] - OpenAIæ ¼å¼çš„å“åº”
            æµå¼: AsyncGenerator[str, None] - SSEæ ¼å¼çš„æµå¼å“åº”
        """
        pass
    
    @abstractmethod
    async def transform_request(self, request: OpenAIRequest) -> Dict[str, Any]:
        """
        è½¬æ¢OpenAIè¯·æ±‚ä¸ºæä¾›å•†ç‰¹å®šæ ¼å¼
        
        Args:
            request: OpenAIæ ¼å¼çš„è¯·æ±‚
            
        Returns:
            Dict[str, Any]: æä¾›å•†ç‰¹å®šæ ¼å¼çš„è¯·æ±‚
        """
        pass
    
    @abstractmethod
    async def transform_response(
        self, 
        response: Any, 
        request: OpenAIRequest
    ) -> Union[Dict[str, Any], AsyncGenerator[str, None]]:
        """
        è½¬æ¢æä¾›å•†å“åº”ä¸ºOpenAIæ ¼å¼
        
        Args:
            response: æä¾›å•†çš„åŸå§‹å“åº”
            request: åŸå§‹è¯·æ±‚ï¼ˆç”¨äºæ„é€ å“åº”ï¼‰
            
        Returns:
            Union[Dict[str, Any], AsyncGenerator[str, None]]: OpenAIæ ¼å¼çš„å“åº”
        """
        pass
    
    def get_supported_models(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        return []
    
    def create_chat_id(self) -> str:
        """ç”ŸæˆèŠå¤©ID"""
        return f"chatcmpl-{uuid.uuid4().hex}"
    
    def create_openai_chunk(
        self, 
        chat_id: str, 
        model: str, 
        delta: Dict[str, Any], 
        finish_reason: Optional[str] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºOpenAIæ ¼å¼çš„æµå¼å“åº”å—"""
        return {
            "id": chat_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": delta,
                "finish_reason": finish_reason,
                "logprobs": None,
            }],
            "system_fingerprint": f"fp_{self.name}_001",
        }
    
    def create_openai_response(
        self, 
        chat_id: str, 
        model: str, 
        content: str, 
        usage: Optional[Dict[str, int]] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºOpenAIæ ¼å¼çš„éæµå¼å“åº”"""
        return {
            "id": chat_id,
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": content
                },
                "finish_reason": "stop",
                "logprobs": None,
            }],
            "usage": usage or {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            },
            "system_fingerprint": f"fp_{self.name}_001",
        }

    def create_openai_response_with_reasoning(
        self,
        chat_id: str,
        model: str,
        content: str,
        reasoning_content: str = None,
        usage: Optional[Dict[str, int]] = None
    ) -> Dict[str, Any]:
        """åˆ›å»ºåŒ…å«æ¨ç†å†…å®¹çš„OpenAIæ ¼å¼éæµå¼å“åº”"""
        message = {
            "role": "assistant",
            "content": content
        }

        # åªæœ‰å½“æ¨ç†å†…å®¹å­˜åœ¨ä¸”ä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ 
        if reasoning_content and reasoning_content.strip():
            message["reasoning_content"] = reasoning_content

        return {
            "id": chat_id,
            "object": "chat.completion",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "message": message,
                "finish_reason": "stop",
                "logprobs": None,
            }],
            "usage": usage or {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            },
            "system_fingerprint": f"fp_{self.name}_001",
        }

    async def format_sse_chunk(self, chunk: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–SSEå“åº”å—"""
        return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
    
    async def format_sse_done(self) -> str:
        """æ ¼å¼åŒ–SSEç»“æŸæ ‡è®°"""
        return "data: [DONE]\n\n"
    
    def log_request(self, request: OpenAIRequest):
        """è®°å½•è¯·æ±‚æ—¥å¿—"""
        self.logger.info(f"ğŸ”„ {self.name} å¤„ç†è¯·æ±‚: {request.model}")
        self.logger.debug(f"  æ¶ˆæ¯æ•°é‡: {len(request.messages)}")
        self.logger.debug(f"  æµå¼æ¨¡å¼: {request.stream}")
        
    def log_response(self, success: bool, error: Optional[str] = None):
        """è®°å½•å“åº”æ—¥å¿—"""
        if success:
            self.logger.info(f"âœ… {self.name} å“åº”æˆåŠŸ")
        else:
            self.logger.error(f"âŒ {self.name} å“åº”å¤±è´¥: {error}")
    
    def handle_error(self, error: Exception, context: str = "") -> Dict[str, Any]:
        """ç»Ÿä¸€é”™è¯¯å¤„ç†"""
        error_msg = f"{self.name} {context} é”™è¯¯: {str(error)}"
        self.logger.error(error_msg)
        
        return {
            "error": {
                "message": error_msg,
                "type": "provider_error",
                "code": "internal_error"
            }
        }


class ProviderRegistry:
    """æä¾›å•†æ³¨å†Œè¡¨"""
    
    def __init__(self):
        self._providers: Dict[str, BaseProvider] = {}
        self._model_mapping: Dict[str, str] = {}
    
    def register(self, provider: BaseProvider, models: List[str]):
        """æ³¨å†Œæä¾›å•†"""
        self._providers[provider.name] = provider
        for model in models:
            self._model_mapping[model] = provider.name
        logger.info(f"ğŸ“ æ³¨å†Œæä¾›å•†: {provider.name}, æ¨¡å‹: {models}")
    
    def get_provider(self, model: str) -> Optional[BaseProvider]:
        """æ ¹æ®æ¨¡å‹è·å–æä¾›å•†"""
        provider_name = self._model_mapping.get(model)
        if provider_name:
            return self._providers.get(provider_name)
        return None
    
    def get_provider_by_name(self, name: str) -> Optional[BaseProvider]:
        """æ ¹æ®åç§°è·å–æä¾›å•†"""
        return self._providers.get(name)
    
    def list_models(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æ”¯æŒçš„æ¨¡å‹"""
        return list(self._model_mapping.keys())
    
    def list_providers(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰æä¾›å•†"""
        return list(self._providers.keys())


# å…¨å±€æä¾›å•†æ³¨å†Œè¡¨
provider_registry = ProviderRegistry()

#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Z.AI æä¾›å•†é€‚é…å™¨
"""

import json
import time
import uuid
import httpx
import asyncio
from datetime import datetime
from typing import Dict, List, Any, Optional, AsyncGenerator, Union

from app.providers.base import BaseProvider, ProviderConfig
from app.models.schemas import OpenAIRequest, Message
from app.core.config import settings
from app.utils.logger import get_logger
from app.utils.token_pool import get_token_pool
from app.core.zai_transformer import (
    get_dynamic_headers,
    generate_uuid
)
from app.utils.sse_tool_handler import SSEToolHandler

logger = get_logger()


class ZAIProvider(BaseProvider):
    """Z.AI æä¾›å•†"""
    
    def __init__(self):
        config = ProviderConfig(
            name="zai",
            api_endpoint=settings.API_ENDPOINT,
            timeout=30,
            headers=get_dynamic_headers()
        )
        super().__init__(config)
        
        # Z.AI ç‰¹å®šé…ç½®
        self.base_url = "https://chat.z.ai"
        self.auth_url = f"{self.base_url}/api/v1/auths/"
        
        # æ¨¡å‹æ˜ å°„
        self.model_mapping = {
            settings.PRIMARY_MODEL: "0727-360B-API",  # GLM-4.5
            settings.THINKING_MODEL: "0727-360B-API",  # GLM-4.5-Thinking
            settings.SEARCH_MODEL: "0727-360B-API",  # GLM-4.5-Search
            settings.AIR_MODEL: "0727-106B-API",  # GLM-4.5-Air
        }
    
    def get_supported_models(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        return [
            settings.PRIMARY_MODEL,
            settings.THINKING_MODEL,
            settings.SEARCH_MODEL,
            settings.AIR_MODEL
        ]
    
    async def get_token(self) -> str:
        """è·å–è®¤è¯ä»¤ç‰Œ"""
        # å¦‚æœå¯ç”¨åŒ¿åæ¨¡å¼ï¼Œåªå°è¯•è·å–è®¿å®¢ä»¤ç‰Œ
        if settings.ANONYMOUS_MODE:
            try:
                headers = get_dynamic_headers()
                async with httpx.AsyncClient() as client:
                    response = await client.get(self.auth_url, headers=headers, timeout=10.0)
                    if response.status_code == 200:
                        data = response.json()
                        token = data.get("token", "")
                        if token:
                            self.logger.debug(f"è·å–è®¿å®¢ä»¤ç‰ŒæˆåŠŸ: {token[:20]}...")
                            return token
            except Exception as e:
                self.logger.warning(f"å¼‚æ­¥è·å–è®¿å®¢ä»¤ç‰Œå¤±è´¥: {e}")

            # åŒ¿åæ¨¡å¼ä¸‹ï¼Œå¦‚æœè·å–è®¿å®¢ä»¤ç‰Œå¤±è´¥ï¼Œç›´æ¥è¿”å›ç©º
            self.logger.error("âŒ åŒ¿åæ¨¡å¼ä¸‹è·å–è®¿å®¢ä»¤ç‰Œå¤±è´¥")
            return ""

        # éåŒ¿åæ¨¡å¼ï¼šé¦–å…ˆä½¿ç”¨tokenæ± è·å–å¤‡ä»½ä»¤ç‰Œ
        token_pool = get_token_pool()
        if token_pool:
            token = token_pool.get_next_token()
            if token:
                self.logger.debug(f"ä»tokenæ± è·å–ä»¤ç‰Œ: {token[:20]}...")
                return token

        # å¦‚æœtokenæ± ä¸ºç©ºæˆ–æ²¡æœ‰å¯ç”¨tokenï¼Œä½¿ç”¨é…ç½®çš„AUTH_TOKEN
        if settings.AUTH_TOKEN and settings.AUTH_TOKEN != "sk-your-api-key":
            self.logger.debug("ä½¿ç”¨é…ç½®çš„AUTH_TOKEN")
            return settings.AUTH_TOKEN

        self.logger.error("âŒ æ— æ³•è·å–æœ‰æ•ˆçš„è®¤è¯ä»¤ç‰Œ")
        return ""
    
    def mark_token_failure(self, token: str, error: Exception = None):
        """æ ‡è®°tokenä½¿ç”¨å¤±è´¥"""
        token_pool = get_token_pool()
        if token_pool:
            token_pool.mark_token_failure(token, error)
    
    async def transform_request(self, request: OpenAIRequest) -> Dict[str, Any]:
        """è½¬æ¢OpenAIè¯·æ±‚ä¸ºZ.AIæ ¼å¼"""
        self.logger.info(f"ğŸ”„ è½¬æ¢ OpenAI è¯·æ±‚åˆ° Z.AI æ ¼å¼: {request.model}")
        
        # è·å–è®¤è¯ä»¤ç‰Œ
        token = await self.get_token()
        
        # å¤„ç†æ¶ˆæ¯æ ¼å¼
        messages = []
        for msg in request.messages:
            if isinstance(msg.content, str):
                messages.append({
                    "role": msg.role,
                    "content": msg.content
                })
            elif isinstance(msg.content, list):
                # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                content_parts = []
                for part in msg.content:
                    if hasattr(part, 'type') and hasattr(part, 'text'):
                        content_parts.append({
                            "type": part.type,
                            "text": part.text
                        })
                messages.append({
                    "role": msg.role,
                    "content": content_parts
                })
        
        # ç¡®å®šè¯·æ±‚çš„æ¨¡å‹ç‰¹æ€§
        requested_model = request.model
        is_thinking = requested_model == settings.THINKING_MODEL
        is_search = requested_model == settings.SEARCH_MODEL
        is_air = requested_model == settings.AIR_MODEL
        
        # è·å–ä¸Šæ¸¸æ¨¡å‹ID
        upstream_model_id = self.model_mapping.get(requested_model, "0727-360B-API")
        
        # æ„å»ºMCPæœåŠ¡å™¨åˆ—è¡¨
        mcp_servers = []
        if is_search:
            mcp_servers.append("deep-web-search")
            self.logger.info("ğŸ” æ£€æµ‹åˆ°æœç´¢æ¨¡å‹ï¼Œæ·»åŠ  deep-web-search MCP æœåŠ¡å™¨")
        
        # æ„å»ºä¸Šæ¸¸è¯·æ±‚ä½“
        chat_id = generate_uuid()
        
        body = {
            "stream": True,  # æ€»æ˜¯ä½¿ç”¨æµå¼
            "model": upstream_model_id,
            "messages": messages,
            "params": {},
            "features": {
                "image_generation": False,
                "web_search": is_search,
                "auto_web_search": is_search,
                "preview_mode": False,
                "flags": [],
                "features": [],
                "enable_thinking": is_thinking,
            },
            "background_tasks": {
                "title_generation": False,
                "tags_generation": False,
            },
            "mcp_servers": mcp_servers,
            "variables": {
                "{{USER_NAME}}": "Guest",
                "{{USER_LOCATION}}": "Unknown",
                "{{CURRENT_DATETIME}}": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "{{CURRENT_DATE}}": datetime.now().strftime("%Y-%m-%d"),
                "{{CURRENT_TIME}}": datetime.now().strftime("%H:%M:%S"),
                "{{CURRENT_WEEKDAY}}": datetime.now().strftime("%A"),
                "{{CURRENT_TIMEZONE}}": "Asia/Shanghai",
                "{{USER_LANGUAGE}}": "zh-CN",
            },
            "model_item": {
                "id": upstream_model_id,
                "name": requested_model,
                "owned_by": "z.ai"
            },
            "chat_id": chat_id,
            "id": generate_uuid(),
        }
        
        # å¤„ç†å·¥å…·æ”¯æŒ
        if settings.TOOL_SUPPORT and not is_thinking and request.tools:
            body["tools"] = request.tools
            self.logger.info(f"å¯ç”¨å·¥å…·æ”¯æŒ: {len(request.tools)} ä¸ªå·¥å…·")
        else:
            body["tools"] = None
        
        # å¤„ç†å…¶ä»–å‚æ•°
        if request.temperature is not None:
            body["params"]["temperature"] = request.temperature
        if request.max_tokens is not None:
            body["params"]["max_tokens"] = request.max_tokens
        
        # æ„å»ºè¯·æ±‚å¤´
        headers = get_dynamic_headers(chat_id)
        if token:
            headers["Authorization"] = f"Bearer {token}"
        
        # å­˜å‚¨å½“å‰tokenç”¨äºé”™è¯¯å¤„ç†
        self._current_token = token

        return {
            "url": self.config.api_endpoint,
            "headers": headers,
            "body": body,
            "token": token,
            "chat_id": chat_id,
            "model": requested_model
        }
    
    async def chat_completion(
        self,
        request: OpenAIRequest,
        **kwargs
    ) -> Union[Dict[str, Any], AsyncGenerator[str, None]]:
        """èŠå¤©å®Œæˆæ¥å£"""
        self.log_request(request)

        try:
            # è½¬æ¢è¯·æ±‚
            transformed = await self.transform_request(request)

            # æ ¹æ®è¯·æ±‚ç±»å‹è¿”å›å“åº”
            if request.stream:
                # æµå¼å“åº”
                return self._create_stream_response_with_retry(request, transformed)
            else:
                # éæµå¼å“åº”
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        transformed["url"],
                        headers=transformed["headers"],
                        json=transformed["body"]
                    )

                    if not response.is_success:
                        error_msg = f"Z.AI API é”™è¯¯: {response.status_code}"
                        self.log_response(False, error_msg)
                        return self.handle_error(Exception(error_msg))

                    return await self.transform_response(response, request, transformed)

        except Exception as e:
            self.log_response(False, str(e))
            return self.handle_error(e, "è¯·æ±‚å¤„ç†")

    async def _create_stream_response_with_retry(
        self,
        request: OpenAIRequest,
        transformed: Dict[str, Any]
    ) -> AsyncGenerator[str, None]:
        """åˆ›å»ºå¸¦é‡è¯•æœºåˆ¶çš„æµå¼å“åº”ç”Ÿæˆå™¨"""
        retry_count = 0
        last_error = None
        current_token = transformed.get("token", "")

        while retry_count <= settings.MAX_RETRIES:
            try:
                # å¦‚æœæ˜¯é‡è¯•ï¼Œé‡æ–°è·å–ä»¤ç‰Œå¹¶æ›´æ–°è¯·æ±‚
                if retry_count > 0:
                    delay = settings.RETRY_DELAY
                    self.logger.warning(f"é‡è¯•è¯·æ±‚ ({retry_count}/{settings.MAX_RETRIES}) - ç­‰å¾… {delay:.1f}s")
                    await asyncio.sleep(delay)

                    # æ ‡è®°å‰ä¸€ä¸ªtokenå¤±è´¥ï¼ˆå¦‚æœä¸æ˜¯åŒ¿åæ¨¡å¼ï¼‰
                    if current_token and not settings.ANONYMOUS_MODE:
                        self.mark_token_failure(current_token, Exception(f"Retry {retry_count}: {last_error}"))

                    # é‡æ–°è·å–ä»¤ç‰Œ
                    self.logger.info("ğŸ”‘ é‡æ–°è·å–ä»¤ç‰Œç”¨äºé‡è¯•...")
                    new_token = await self.get_token()
                    if not new_token:
                        self.logger.error("âŒ é‡è¯•æ—¶æ— æ³•è·å–æœ‰æ•ˆçš„è®¤è¯ä»¤ç‰Œ")
                        raise Exception("é‡è¯•æ—¶æ— æ³•è·å–æœ‰æ•ˆçš„è®¤è¯ä»¤ç‰Œ")
                    transformed["headers"]["Authorization"] = f"Bearer {new_token}"
                    current_token = new_token

                async with httpx.AsyncClient(timeout=60.0) as client:
                    # å‘é€è¯·æ±‚åˆ°ä¸Šæ¸¸
                    self.logger.info(f"ğŸ¯ å‘é€è¯·æ±‚åˆ° Z.AI: {transformed['url']}")
                    async with client.stream(
                        "POST",
                        transformed["url"],
                        json=transformed["body"],
                        headers=transformed["headers"],
                    ) as response:
                        # æ£€æŸ¥å“åº”çŠ¶æ€ç 
                        if response.status_code == 400:
                            # 400 é”™è¯¯ï¼Œè§¦å‘é‡è¯•
                            error_text = await response.aread()
                            error_msg = error_text.decode('utf-8', errors='ignore')
                            self.logger.warning(f"âŒ ä¸Šæ¸¸è¿”å› 400 é”™è¯¯ (å°è¯• {retry_count + 1}/{settings.MAX_RETRIES + 1})")

                            retry_count += 1
                            last_error = f"400 Bad Request: {error_msg}"

                            # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç»§ç»­å¾ªç¯
                            if retry_count <= settings.MAX_RETRIES:
                                continue
                            else:
                                # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒæŠ›å‡ºé”™è¯¯
                                self.logger.error(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({settings.MAX_RETRIES})ï¼Œè¯·æ±‚å¤±è´¥")
                                error_response = {
                                    "error": {
                                        "message": f"Request failed after {settings.MAX_RETRIES} retries: {last_error}",
                                        "type": "upstream_error",
                                        "code": 400
                                    }
                                }
                                yield f"data: {json.dumps(error_response)}\n\n"
                                yield "data: [DONE]\n\n"
                                return

                        elif response.status_code != 200:
                            # å…¶ä»–é”™è¯¯ï¼Œç›´æ¥è¿”å›
                            self.logger.error(f"âŒ ä¸Šæ¸¸è¿”å›é”™è¯¯: {response.status_code}")
                            error_text = await response.aread()
                            error_msg = error_text.decode('utf-8', errors='ignore')
                            self.logger.error(f"âŒ é”™è¯¯è¯¦æƒ…: {error_msg}")

                            error_response = {
                                "error": {
                                    "message": f"Upstream error: {response.status_code}",
                                    "type": "upstream_error",
                                    "code": response.status_code
                                }
                            }
                            yield f"data: {json.dumps(error_response)}\n\n"
                            yield "data: [DONE]\n\n"
                            return

                        # 200 æˆåŠŸï¼Œå¤„ç†å“åº”
                        if retry_count > 0:
                            self.logger.info(f"âœ¨ ç¬¬ {retry_count} æ¬¡é‡è¯•æˆåŠŸ")

                        # æ ‡è®°tokenä½¿ç”¨æˆåŠŸï¼ˆå¦‚æœä¸æ˜¯åŒ¿åæ¨¡å¼ï¼‰
                        if current_token and not settings.ANONYMOUS_MODE:
                            token_pool = get_token_pool()
                            if token_pool:
                                token_pool.mark_token_success(current_token)

                        # å¤„ç†æµå¼å“åº”
                        chat_id = transformed["chat_id"]
                        model = transformed["model"]
                        async for chunk in self._handle_stream_response(response, chat_id, model, request, transformed):
                            yield chunk
                        return

            except Exception as e:
                self.logger.error(f"âŒ æµå¤„ç†é”™è¯¯: {e}")
                import traceback
                self.logger.error(traceback.format_exc())

                # æ ‡è®°tokenå¤±è´¥ï¼ˆå¦‚æœä¸æ˜¯åŒ¿åæ¨¡å¼ï¼‰
                if current_token and not settings.ANONYMOUS_MODE:
                    self.mark_token_failure(current_token, e)

                # æ£€æŸ¥æ˜¯å¦è¿˜å¯ä»¥é‡è¯•
                retry_count += 1
                last_error = str(e)

                if retry_count > settings.MAX_RETRIES:
                    # è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›é”™è¯¯
                    self.logger.error(f"âŒ è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({settings.MAX_RETRIES})ï¼Œæµå¤„ç†å¤±è´¥")
                    error_response = {
                        "error": {
                            "message": f"Stream processing failed after {settings.MAX_RETRIES} retries: {last_error}",
                            "type": "stream_error"
                        }
                    }
                    yield f"data: {json.dumps(error_response)}\n\n"
                    yield "data: [DONE]\n\n"
                    return
    
    async def transform_response(
        self, 
        response: httpx.Response, 
        request: OpenAIRequest,
        transformed: Dict[str, Any]
    ) -> Union[Dict[str, Any], AsyncGenerator[str, None]]:
        """è½¬æ¢Z.AIå“åº”ä¸ºOpenAIæ ¼å¼"""
        chat_id = transformed["chat_id"]
        model = transformed["model"]
        
        if request.stream:
            return self._handle_stream_response(response, chat_id, model, request, transformed)
        else:
            return await self._handle_non_stream_response(response, chat_id, model)
    
    async def _handle_stream_response(
        self,
        response: httpx.Response,
        chat_id: str,
        model: str,
        request: OpenAIRequest,
        transformed: Dict[str, Any]
    ) -> AsyncGenerator[str, None]:
        """å¤„ç†Z.AIæµå¼å“åº”"""
        self.logger.info(f"âœ… Z.AI å“åº”æˆåŠŸï¼Œå¼€å§‹å¤„ç† SSE æµ")

        # åˆå§‹åŒ–å·¥å…·å¤„ç†å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        has_tools = transformed["body"].get("tools") is not None
        tool_handler = None

        if has_tools:
            tool_handler = SSEToolHandler(model, stream=True)
            self.logger.info(f"ğŸ”§ åˆå§‹åŒ–å·¥å…·å¤„ç†å™¨: {len(transformed['body'].get('tools', []))} ä¸ªå·¥å…·")

        # å¤„ç†çŠ¶æ€
        has_thinking = False
        thinking_signature = None

        # å¤„ç†SSEæµ
        buffer = ""
        line_count = 0
        self.logger.debug("ğŸ“¡ å¼€å§‹æ¥æ”¶ SSE æµæ•°æ®...")

        try:
            async for line in response.aiter_lines():
                line_count += 1
                if not line:
                    continue

                # ç´¯ç§¯åˆ°bufferå¤„ç†å®Œæ•´çš„æ•°æ®è¡Œ
                buffer += line + "\n"

                # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„dataè¡Œ
                while "\n" in buffer:
                    current_line, buffer = buffer.split("\n", 1)
                    if not current_line.strip():
                        continue

                    if current_line.startswith("data:"):
                        chunk_str = current_line[5:].strip()
                        if not chunk_str or chunk_str == "[DONE]":
                            if chunk_str == "[DONE]":
                                yield "data: [DONE]\n\n"
                            continue

                        self.logger.debug(f"ğŸ“¦ è§£ææ•°æ®å—: {chunk_str[:1000]}..." if len(chunk_str) > 1000 else f"ğŸ“¦ è§£ææ•°æ®å—: {chunk_str}")

                        try:
                            chunk = json.loads(chunk_str)

                            if chunk.get("type") == "chat:completion":
                                data = chunk.get("data", {})
                                phase = data.get("phase")

                                # è®°å½•æ¯ä¸ªé˜¶æ®µï¼ˆåªåœ¨é˜¶æ®µå˜åŒ–æ—¶è®°å½•ï¼‰
                                if phase and phase != getattr(self, '_last_phase', None):
                                    self.logger.info(f"ğŸ“ˆ SSE é˜¶æ®µ: {phase}")
                                    self._last_phase = phase

                                # ä½¿ç”¨å·¥å…·å¤„ç†å™¨å¤„ç†æ‰€æœ‰é˜¶æ®µ
                                if tool_handler:
                                    # æ„å»º SSE æ•°æ®å—ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦å­—æ®µ
                                    sse_chunk = {
                                        "phase": phase,
                                        "edit_content": data.get("edit_content", ""),
                                        "delta_content": data.get("delta_content", ""),
                                        "edit_index": data.get("edit_index"),
                                        "usage": data.get("usage", {})
                                    }

                                    # å¤„ç†å·¥å…·è°ƒç”¨å¹¶è¾“å‡ºç»“æœ
                                    for output in tool_handler.process_sse_chunk(sse_chunk):
                                        yield output

                                # éå·¥å…·è°ƒç”¨æ¨¡å¼ - å¤„ç†æ€è€ƒå†…å®¹
                                elif phase == "thinking":
                                    if not has_thinking:
                                        has_thinking = True
                                        # å‘é€åˆå§‹è§’è‰²
                                        role_chunk = self.create_openai_chunk(
                                            chat_id,
                                            model,
                                            {"role": "assistant"}
                                        )
                                        yield await self.format_sse_chunk(role_chunk)

                                    delta_content = data.get("delta_content", "")
                                    if delta_content:
                                        # å¤„ç†æ€è€ƒå†…å®¹æ ¼å¼
                                        if delta_content.startswith("<details"):
                                            content = (
                                                delta_content.split("</summary>\n>")[-1].strip()
                                                if "</summary>\n>" in delta_content
                                                else delta_content
                                            )
                                        else:
                                            content = delta_content

                                        thinking_chunk = self.create_openai_chunk(
                                            chat_id,
                                            model,
                                            {
                                                "role": "assistant",
                                                "thinking": {"content": content}
                                            }
                                        )
                                        yield await self.format_sse_chunk(thinking_chunk)

                                # å¤„ç†ç­”æ¡ˆå†…å®¹
                                elif phase == "answer":
                                    edit_content = data.get("edit_content", "")
                                    delta_content = data.get("delta_content", "")

                                    # å¤„ç†æ€è€ƒç»“æŸå’Œç­”æ¡ˆå¼€å§‹
                                    if edit_content and "</details>\n" in edit_content:
                                        if has_thinking:
                                            # å‘é€æ€è€ƒç­¾å
                                            thinking_signature = str(int(time.time() * 1000))
                                            sig_chunk = self.create_openai_chunk(
                                                chat_id,
                                                model,
                                                {
                                                    "role": "assistant",
                                                    "thinking": {
                                                        "content": "",
                                                        "signature": thinking_signature,
                                                    }
                                                }
                                            )
                                            yield await self.format_sse_chunk(sig_chunk)

                                        # æå–ç­”æ¡ˆå†…å®¹
                                        content_after = edit_content.split("</details>\n")[-1]
                                        if content_after:
                                            content_chunk = self.create_openai_chunk(
                                                chat_id,
                                                model,
                                                {
                                                    "role": "assistant",
                                                    "content": content_after
                                                }
                                            )
                                            yield await self.format_sse_chunk(content_chunk)

                                    # å¤„ç†å¢é‡å†…å®¹
                                    elif delta_content:
                                        # å¦‚æœè¿˜æ²¡æœ‰å‘é€è§’è‰²
                                        if not has_thinking:
                                            role_chunk = self.create_openai_chunk(
                                                chat_id,
                                                model,
                                                {"role": "assistant"}
                                            )
                                            yield await self.format_sse_chunk(role_chunk)

                                        content_chunk = self.create_openai_chunk(
                                            chat_id,
                                            model,
                                            {
                                                "role": "assistant",
                                                "content": delta_content
                                            }
                                        )
                                        output_data = await self.format_sse_chunk(content_chunk)
                                        self.logger.debug(f"â¡ï¸ è¾“å‡ºå†…å®¹å—åˆ°å®¢æˆ·ç«¯: {output_data}")
                                        yield output_data

                                    # å¤„ç†å®Œæˆ
                                    if data.get("usage"):
                                        self.logger.info(f"ğŸ“¦ å®Œæˆå“åº” - ä½¿ç”¨ç»Ÿè®¡: {json.dumps(data['usage'])}")

                                        # åªæœ‰åœ¨éå·¥å…·è°ƒç”¨æ¨¡å¼ä¸‹æ‰å‘é€æ™®é€šå®Œæˆä¿¡å·
                                        if not tool_handler:
                                            finish_chunk = self.create_openai_chunk(
                                                chat_id,
                                                model,
                                                {"role": "assistant", "content": ""},
                                                "stop"
                                            )
                                            finish_chunk["usage"] = data["usage"]

                                            finish_output = await self.format_sse_chunk(finish_chunk)
                                            self.logger.debug(f"â¡ï¸ å‘é€å®Œæˆä¿¡å·: {finish_output[:1000]}...")
                                            yield finish_output
                                            self.logger.debug("â¡ï¸ å‘é€ [DONE]")
                                            yield "data: [DONE]\n\n"

                        except json.JSONDecodeError as e:
                            self.logger.debug(f"âŒ JSONè§£æé”™è¯¯: {e}, å†…å®¹: {chunk_str[:1000]}")
                        except Exception as e:
                            self.logger.error(f"âŒ å¤„ç†chunké”™è¯¯: {e}")

            # å·¥å…·å¤„ç†å™¨ä¼šè‡ªåŠ¨å‘é€ç»“æŸä¿¡å·ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤å‘é€
            if not tool_handler:
                self.logger.debug("ğŸ“¤ å‘é€æœ€ç»ˆ [DONE] ä¿¡å·")
                yield "data: [DONE]\n\n"

            self.logger.info(f"âœ… SSE æµå¤„ç†å®Œæˆï¼Œå…±å¤„ç† {line_count} è¡Œæ•°æ®")

        except Exception as e:
            self.logger.error(f"âŒ æµå¼å“åº”å¤„ç†é”™è¯¯: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            # å‘é€é”™è¯¯ç»“æŸå—
            yield await self.format_sse_chunk(
                self.create_openai_chunk(chat_id, model, {}, "stop")
            )
            yield "data: [DONE]\n\n"
    
    async def _handle_non_stream_response(
        self, 
        response: httpx.Response, 
        chat_id: str, 
        model: str
    ) -> Dict[str, Any]:
        """å¤„ç†éæµå¼å“åº”"""
        # ç®€åŒ–çš„éæµå¼å“åº”å¤„ç†
        content = "éæµå¼å“åº”å¤„ç†ä¸­..."
        return self.create_openai_response(chat_id, model, content)

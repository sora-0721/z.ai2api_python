#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
K2Think æä¾›å•†é€‚é…å™¨
åŸºäº backup/cof.js ä¸­çš„ K2Think é€»è¾‘å®ç°
"""

import json
import re
import time
import uuid
import httpx
from typing import Dict, List, Any, Optional, AsyncGenerator, Union

from app.providers.base import BaseProvider, ProviderConfig
from app.models.schemas import OpenAIRequest, Message
from app.utils.logger import get_logger

logger = get_logger()


class K2ThinkProvider(BaseProvider):
    """K2Think æä¾›å•†"""
    
    def __init__(self):
        config = ProviderConfig(
            name="k2think",
            api_endpoint="https://www.k2think.ai/api/guest/chat/completions",
            timeout=30,
            headers={
                'Accept': 'text/event-stream',
                'Accept-Encoding': 'gzip, deflate, br, zstd',
                'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
                'Content-Type': 'application/json',
                'Origin': 'https://www.k2think.ai',
                'Pragma': 'no-cache',
                'Referer': 'https://www.k2think.ai/guest',
                'Sec-Ch-Ua': '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"macOS"',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin',
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
            }
        )
        super().__init__(config)

        # K2Think ç‰¹å®šé…ç½®
        self.handshake_url = "https://www.k2think.ai/guest"
        self.new_chat_url = "https://www.k2think.ai/api/v1/chats/guest/new"
        
        # å†…å®¹è§£ææ­£åˆ™è¡¨è¾¾å¼ - ä½¿ç”¨DOTALLæ ‡å¿—ç¡®ä¿.åŒ¹é…æ¢è¡Œç¬¦
        self.reasoning_pattern = re.compile(r'<details type="reasoning"[^>]*>.*?<summary>.*?</summary>(.*?)</details>', re.DOTALL)
        self.answer_pattern = re.compile(r'<answer>(.*?)</answer>', re.DOTALL)
    
    def get_supported_models(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        return ["MBZUAI-IFM/K2-Think"]
    
    def parse_cookies(self, headers) -> str:
        """è§£æCookie"""
        cookies = []
        for key, value in headers.items():
            if key.lower() == 'set-cookie':
                cookies.append(value.split(';')[0])
        return '; '.join(cookies)
    
    def extract_reasoning_and_answer(self, content: str) -> tuple[str, str]:
        """æå–æ¨ç†å†…å®¹å’Œç­”æ¡ˆå†…å®¹"""
        if not content:
            return "", ""
        
        try:
            reasoning_match = self.reasoning_pattern.search(content)
            reasoning = reasoning_match.group(1).strip() if reasoning_match else ""
            
            answer_match = self.answer_pattern.search(content)
            answer = answer_match.group(1).strip() if answer_match else ""
            
            return reasoning, answer
        except Exception as e:
            self.logger.error(f"æå–K2å†…å®¹é”™è¯¯: {e}")
            return "", ""
    
    def calculate_delta(self, previous: str, current: str) -> str:
        """è®¡ç®—å†…å®¹å¢é‡"""
        if not previous:
            return current
        if not current or len(current) < len(previous):
            return ""
        return current[len(previous):]
    
    def parse_api_response(self, obj: Any) -> tuple[str, bool]:
        """è§£æAPIå“åº”"""
        if not obj or not isinstance(obj, dict):
            return "", False
        
        if obj.get("done") is True:
            return "", True
        
        choices = obj.get("choices", [])
        if choices and len(choices) > 0:
            delta = choices[0].get("delta", {})
            return delta.get("content", ""), False
        
        content = obj.get("content")
        if isinstance(content, str):
            return content, False
        
        return "", False
    
    async def get_k2_auth_data(self, request: OpenAIRequest) -> Dict[str, Any]:
        """è·å–K2Thinkè®¤è¯æ•°æ®"""
        # 1. æ¡æ‰‹è¯·æ±‚ - ä½¿ç”¨æ›´ç®€å•çš„Accept-Encodingæ¥é¿å…Brotlié—®é¢˜
        headers_for_handshake = {**self.config.headers}
        headers_for_handshake['Accept-Encoding'] = 'gzip, deflate'  # ç§»é™¤brå’Œzstd

        async with httpx.AsyncClient() as client:
            handshake_response = await client.get(
                self.handshake_url,
                headers=headers_for_handshake,
                follow_redirects=True
            )
            if not handshake_response.is_success:
                try:
                    # ä½¿ç”¨httpxçš„textå±æ€§ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†è§£å‹ç¼©å’Œç¼–ç 
                    error_text = handshake_response.text
                    raise Exception(f"K2 æ¡æ‰‹å¤±è´¥: {handshake_response.status_code} {error_text[:200]}")
                except Exception as e:
                    raise Exception(f"K2 æ¡æ‰‹å¤±è´¥: {handshake_response.status_code}")
            
            initial_cookies = self.parse_cookies(handshake_response.headers)
        
        # 2. å‡†å¤‡æ¶ˆæ¯
        prepared_messages = self.prepare_k2_messages(request.messages)
        first_user_message = next((m for m in prepared_messages if m["role"] == "user"), None)
        if not first_user_message:
            raise Exception("æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯æ¥åˆå§‹åŒ–å¯¹è¯")
        
        # 3. åˆ›å»ºæ–°å¯¹è¯
        message_id = str(uuid.uuid4())
        now = int(time.time() * 1000)
        model_id = request.model or "MBZUAI-IFM/K2-Think"
        
        new_chat_payload = {
            "chat": {
                "id": "",
                "title": "Guest Chat",
                "models": [model_id],
                "params": {},
                "history": {
                    "messages": {
                        message_id: {
                            "id": message_id,
                            "parentId": None,
                            "childrenIds": [],
                            "role": "user",
                            "content": first_user_message["content"],
                            "timestamp": now // 1000,
                            "models": [model_id]
                        }
                    },
                    "currentId": message_id
                },
                "messages": [{
                    "id": message_id,
                    "parentId": None,
                    "childrenIds": [],
                    "role": "user",
                    "content": first_user_message["content"],
                    "timestamp": now // 1000,
                    "models": [model_id]
                }],
                "tags": [],
                "timestamp": now
            }
        }
        
        headers_with_cookies = {**self.config.headers, 'Cookie': initial_cookies}
        headers_with_cookies['Accept-Encoding'] = 'gzip, deflate'  # ç§»é™¤brå’Œzstd

        async with httpx.AsyncClient() as client:
            new_chat_response = await client.post(
                self.new_chat_url,
                headers=headers_with_cookies,
                json=new_chat_payload,
                follow_redirects=True
            )
            if not new_chat_response.is_success:
                try:
                    # ä½¿ç”¨httpxçš„textå±æ€§ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†è§£å‹ç¼©å’Œç¼–ç 
                    error_text = new_chat_response.text
                except Exception:
                    error_text = f"Status: {new_chat_response.status_code}"
                raise Exception(f"K2 æ–°å¯¹è¯åˆ›å»ºå¤±è´¥: {new_chat_response.status_code} {error_text[:200]}")

            try:
                new_chat_data = new_chat_response.json()
            except Exception as e:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•è·å–åŸå§‹å†…å®¹
                try:
                    # ä½¿ç”¨httpxçš„textå±æ€§ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†è§£å‹ç¼©å’Œç¼–ç 
                    content_str = new_chat_response.text
                    self.logger.debug(f"K2 å“åº”åŸå§‹å†…å®¹: {content_str[:500]}")
                    raise Exception(f"K2 å“åº”JSONè§£æå¤±è´¥: {e}, åŸå§‹å†…å®¹: {content_str[:200]}")
                except Exception as decode_error:
                    # å¦‚æœtextä¹Ÿå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨å¤„ç†
                    try:
                        raw_bytes = new_chat_response.content
                        content_str = raw_bytes.decode('utf-8', errors='replace')
                        raise Exception(f"K2 å“åº”è§£æå¤±è´¥: {e}, æ‰‹åŠ¨è§£ç å†…å®¹: {content_str[:200]}")
                    except Exception:
                        raise Exception(f"K2 å“åº”è§£æå®Œå…¨å¤±è´¥: {e}, è§£ç é”™è¯¯: {decode_error}")
            conversation_id = new_chat_data.get("id")
            if not conversation_id:
                raise Exception("æ— æ³•ä»K2 /newç«¯ç‚¹è·å–conversation_id")
            
            chat_specific_cookies = self.parse_cookies(new_chat_response.headers)
        
        # 4. ç»„åˆæœ€ç»ˆCookie
        base_cookies = [initial_cookies, chat_specific_cookies]
        base_cookies = [c for c in base_cookies if c]
        final_cookie = '; '.join(base_cookies) + '; guest_conversation_count=1'
        
        # 5. æ„å»ºæœ€ç»ˆè¯·æ±‚è½½è·
        final_payload = {
            "stream": True,
            "model": model_id,
            "messages": prepared_messages,
            "conversation_id": conversation_id,
            "params": {}
        }
        
        # æ·»åŠ å¯é€‰å‚æ•°
        if request.temperature is not None:
            final_payload["params"]["temperature"] = request.temperature
        if request.max_tokens is not None:
            final_payload["params"]["max_tokens"] = request.max_tokens
        
        final_headers = {**self.config.headers, 'Cookie': final_cookie}
        
        return {
            "payload": final_payload,
            "headers": final_headers
        }
    
    def prepare_k2_messages(self, messages: List[Message]) -> List[Dict[str, Any]]:
        """å‡†å¤‡K2Thinkæ¶ˆæ¯æ ¼å¼"""
        result = []
        system_content = ""
        
        for msg in messages:
            if msg.role == "system":
                system_content = system_content + "\n\n" + msg.content if system_content else msg.content
            else:
                content = msg.content
                if isinstance(content, list):
                    # å¤„ç†å¤šæ¨¡æ€å†…å®¹ï¼Œæå–æ–‡æœ¬
                    text_parts = [part.text for part in content if hasattr(part, 'text') and part.text]
                    content = "\n".join(text_parts)
                
                result.append({
                    "role": msg.role,
                    "content": content
                })
        
        # å°†ç³»ç»Ÿæ¶ˆæ¯åˆå¹¶åˆ°ç¬¬ä¸€ä¸ªç”¨æˆ·æ¶ˆæ¯ä¸­
        if system_content:
            first_user_idx = next((i for i, m in enumerate(result) if m["role"] == "user"), -1)
            if first_user_idx >= 0:
                result[first_user_idx]["content"] = f"{system_content}\n\n{result[first_user_idx]['content']}"
            else:
                result.insert(0, {"role": "user", "content": system_content})
        
        return result

    async def _handle_stream_request(
        self,
        transformed: Dict[str, Any],
        request: OpenAIRequest
    ) -> AsyncGenerator[str, None]:
        """å¤„ç†æµå¼è¯·æ±‚ - åœ¨client.streamä¸Šä¸‹æ–‡å†…ç›´æ¥å¤„ç†"""
        chat_id = self.create_chat_id()
        model = transformed["model"]

        # å‡†å¤‡è¯·æ±‚å¤´
        headers_for_request = {**transformed["headers"]}
        headers_for_request['Accept-Encoding'] = 'gzip, deflate'

        self.logger.info(f"ğŸŒŠ å¼€å§‹K2Thinkæµå¼è¯·æ±‚")

        async with httpx.AsyncClient(timeout=30.0) as client:
            async with client.stream(
                "POST",
                transformed["url"],
                headers=headers_for_request,
                json=transformed["payload"]
            ) as response:
                if not response.is_success:
                    error_msg = f"K2Think API é”™è¯¯: {response.status_code}"
                    self.log_response(False, error_msg)
                    # å¯¹äºæµå¼å“åº”ï¼Œæˆ‘ä»¬éœ€è¦yieldé”™è¯¯ä¿¡æ¯
                    yield await self.format_sse_chunk({
                        "error": {
                            "message": error_msg,
                            "type": "provider_error",
                            "code": "api_error"
                        }
                    })
                    return

                # å‘é€åˆå§‹è§’è‰²å—
                yield await self.format_sse_chunk(
                    self.create_openai_chunk(chat_id, model, {"role": "assistant"})
                )

                # å¤„ç†æµå¼æ•°æ®
                accumulated_content = ""
                previous_reasoning = ""
                previous_answer = ""
                reasoning_phase = True
                chunk_count = 0

                try:
                    async for line in response.aiter_lines():
                        chunk_count += 1
                        self.logger.debug(f"ğŸ“¦ æ”¶åˆ°æ•°æ®å— #{chunk_count}: {line[:100]}...")

                        if not line.startswith("data:"):
                            continue

                        data_str = line[5:].strip()
                        if self._is_end_marker(data_str):
                            self.logger.debug(f"ğŸ æ£€æµ‹åˆ°ç»“æŸæ ‡è®°: {data_str}")
                            continue

                        content = self._parse_data_string(data_str)
                        if not content:
                            continue

                        accumulated_content = content
                        current_reasoning, current_answer = self.extract_reasoning_and_answer(accumulated_content)

                        # å¤„ç†æ¨ç†é˜¶æ®µ
                        if reasoning_phase and current_reasoning:
                            delta = self.calculate_delta(previous_reasoning, current_reasoning)
                            if delta.strip():
                                self.logger.debug(f"ğŸ§  æ¨ç†å¢é‡: {delta[:50]}...")
                                yield await self.format_sse_chunk(
                                    self.create_openai_chunk(chat_id, model, {"reasoning_content": delta})
                                )
                                previous_reasoning = current_reasoning

                        # åˆ‡æ¢åˆ°ç­”æ¡ˆé˜¶æ®µ
                        if current_answer and reasoning_phase:
                            reasoning_phase = False
                            self.logger.debug("ğŸ”„ åˆ‡æ¢åˆ°ç­”æ¡ˆé˜¶æ®µ")
                            # å‘é€å‰©ä½™çš„æ¨ç†å†…å®¹
                            final_reasoning_delta = self.calculate_delta(previous_reasoning, current_reasoning)
                            if final_reasoning_delta.strip():
                                yield await self.format_sse_chunk(
                                    self.create_openai_chunk(chat_id, model, {"reasoning_content": final_reasoning_delta})
                                )

                        # å¤„ç†ç­”æ¡ˆé˜¶æ®µ
                        if not reasoning_phase and current_answer:
                            delta = self.calculate_delta(previous_answer, current_answer)
                            if delta.strip():
                                self.logger.debug(f"ğŸ’¬ ç­”æ¡ˆå¢é‡: {delta[:50]}...")
                                yield await self.format_sse_chunk(
                                    self.create_openai_chunk(chat_id, model, {"content": delta})
                                )
                                previous_answer = current_answer

                except Exception as e:
                    self.logger.error(f"æµå¼å“åº”å¤„ç†é”™è¯¯: {e}")
                    yield await self.format_sse_chunk({
                        "error": {
                            "message": f"æµå¼å¤„ç†é”™è¯¯: {str(e)}",
                            "type": "stream_error",
                            "code": "processing_error"
                        }
                    })
                    return

                # å‘é€ç»“æŸå—
                self.logger.info(f"âœ… K2Thinkæµå¼å“åº”å®Œæˆï¼Œå…±å¤„ç† {chunk_count} ä¸ªæ•°æ®å—")
                yield await self.format_sse_chunk(
                    self.create_openai_chunk(chat_id, model, {}, "stop")
                )
                yield await self.format_sse_done()

    async def transform_request(self, request: OpenAIRequest) -> Dict[str, Any]:
        """è½¬æ¢OpenAIè¯·æ±‚ä¸ºK2Thinkæ ¼å¼"""
        self.logger.info(f"ğŸ”„ è½¬æ¢ OpenAI è¯·æ±‚åˆ° K2Think æ ¼å¼: {request.model}")
        
        auth_data = await self.get_k2_auth_data(request)
        
        return {
            "url": self.config.api_endpoint,
            "headers": auth_data["headers"],
            "payload": auth_data["payload"],
            "model": request.model
        }
    
    async def chat_completion(
        self,
        request: OpenAIRequest
    ) -> Union[Dict[str, Any], AsyncGenerator[str, None]]:
        """èŠå¤©å®Œæˆæ¥å£"""
        self.log_request(request)

        try:
            # è½¬æ¢è¯·æ±‚
            transformed = await self.transform_request(request)

            # å‘é€è¯·æ±‚ - ä½¿ç”¨æ›´å…¼å®¹çš„å‹ç¼©è®¾ç½®
            headers_for_request = {**transformed["headers"]}
            headers_for_request['Accept-Encoding'] = 'gzip, deflate'  # ç§»é™¤brå’Œzstd

            if request.stream:
                # æµå¼è¯·æ±‚ - ç›´æ¥åœ¨è¿™é‡Œå¤„ç†æµå¼å“åº”
                return self._handle_stream_request(transformed, request)
            else:
                # éæµå¼è¯·æ±‚ - ä½¿ç”¨ä¼ ç»Ÿçš„ client.post()
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        transformed["url"],
                        headers=headers_for_request,
                        json=transformed["payload"]
                    )

                    if not response.is_success:
                        error_msg = f"K2Think API é”™è¯¯: {response.status_code}"
                        self.log_response(False, error_msg)
                        return self.handle_error(Exception(error_msg))

                    # è½¬æ¢éæµå¼å“åº”
                    return await self.transform_response(response, request, transformed)

        except Exception as e:
            self.log_response(False, str(e))
            return self.handle_error(e, "è¯·æ±‚å¤„ç†")
    
    async def transform_response(
        self,
        response: httpx.Response,
        request: OpenAIRequest,
        transformed: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è½¬æ¢K2Thinkå“åº”ä¸ºOpenAIæ ¼å¼ - ä»…ç”¨äºéæµå¼è¯·æ±‚"""
        chat_id = self.create_chat_id()
        model = transformed["model"]

        # æµå¼è¯·æ±‚ç°åœ¨ç”± _handle_stream_request ç›´æ¥å¤„ç†
        # è¿™é‡Œåªå¤„ç†éæµå¼è¯·æ±‚
        return await self._handle_non_stream_response(response, chat_id, model)

    def _is_end_marker(self, data: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç»“æŸæ ‡è®°"""
        return not data or data in ["-1", "[DONE]", "DONE", "done"]
    
    def _parse_data_string(self, data_str: str) -> str:
        """è§£ææ•°æ®å­—ç¬¦ä¸²"""
        try:
            obj = json.loads(data_str)
            content, is_done = self.parse_api_response(obj)
            return "" if is_done else content
        except:
            return data_str
    
    async def _handle_non_stream_response(
        self,
        response: httpx.Response,
        chat_id: str,
        model: str
    ) -> Dict[str, Any]:
        """å¤„ç†K2Thinkéæµå¼å“åº”"""
        # èšåˆæµå¼å†…å®¹ - ä½¿ç”¨httpxçš„aiter_linesï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†è§£å‹ç¼©
        final_content = ""

        try:
            # ä½¿ç”¨aiter_lines()ï¼Œhttpxä¼šè‡ªåŠ¨å¤„ç†å‹ç¼©å’Œç¼–ç 
            async for line in response.aiter_lines():
                if not line.startswith("data:"):
                    continue

                data_str = line[5:].strip()
                if self._is_end_marker(data_str):
                    continue

                content = self._parse_data_string(data_str)
                if content:
                    final_content = content

        except Exception as e:
            self.logger.error(f"éæµå¼å“åº”å¤„ç†é”™è¯¯: {e}")
            raise

        # æå–æ¨ç†å†…å®¹å’Œç­”æ¡ˆå†…å®¹
        reasoning, answer = self.extract_reasoning_and_answer(final_content)

        # æ¸…ç†å†…å®¹æ ¼å¼
        reasoning = reasoning.replace("\\n", "\n") if reasoning else ""
        answer = answer.replace("\\n", "\n") if answer else final_content

        # åˆ›å»ºåŒ…å«æ¨ç†å†…å®¹çš„å“åº”
        return self.create_openai_response_with_reasoning(chat_id, model, answer, reasoning)
